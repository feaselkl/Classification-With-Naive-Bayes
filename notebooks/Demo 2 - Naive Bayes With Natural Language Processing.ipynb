{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes With Natural Language Processing\n",
    "\n",
    "In this demo, we will look at using the `tm` (text mining) package to perform Natural Language Processing (NLP) on a data set and generate a Naive Bayes model which performs sentiment analysis.\n",
    "\n",
    "To do this, we will need four packages:\n",
    "* The `naivebayes` package will help us generate our model.\n",
    "* The `tidyverse` package gives us the `magrittr` pipe.  In a real scenario, we'd probably use other `tidyverse` functionality like `dplyr` functions or `ggplot2` graphics.\n",
    "* The `tm` package allows us to perform text mining activities.  An alternative package would be `tidytext`.\n",
    "* The `caret` package gives us a good confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(!require(naivebayes)) {\n",
    "  install.packages(\"naivebayes\")\n",
    "  library(naivebayes)\n",
    "}\n",
    "if(!require(tidyverse)) {\n",
    "  install.packages(\"tidyverse\")\n",
    "  library(tidyverse)\n",
    "}\n",
    "if(!require(tm)) {\n",
    "  install.packages(\"tm\")\n",
    "  library(tm)\n",
    "}\n",
    "if(!require(caret)) {\n",
    "  install.packages(\"caret\")\n",
    "  library(caret)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data set is a sample of 2000 movie reviews.  There are two variables in this data set:  a **class** feature which is \"Pos\" (for a positive review) and \"Neg\" (for a negative review).  Then, **text** is the review itself.\n",
    "\n",
    "The data set is still a bit messy, so we will have to do some work on it to simplify processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df <- readr::read_csv(\"../data/movie-pang02.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glimpse(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in the first demo, the data set size is small enough that I don't mind shuffling data this way.  I do want to make sure that **class** is a factor, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1)\n",
    "df <- df[sample(nrow(df)),]\n",
    "df <- df[sample(nrow(df)),]\n",
    "df$class <- as.factor(df$class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step after shuffling data is to build a corpus.  This is the full set of data.\n",
    "\n",
    "**NOTE:** in a real data science project, I would *not* build the corpus here; I'd split into training and test data sets.  Then I'd build a corpus for the training set and apply those same rules to the test data set independently.  The problem is that you have the potential of some information from the test set leaking into the training set, which might cause overfitting against the test set.  This is troublesome because you'll think you have a good accuracy, but accuracy might drop considerably when moving to real-world data. \n",
    "\n",
    "The reason why we might think about doing this, though, is that it simplifies the document term matrix that we end up creating--it prevents the possibility of a term showing up in the test data set which did not appear in the original DTM we used for training.  This is a sloppy way of getting around a potential error during testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus <- tm::Corpus(tm::VectorSource(df$text))\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are applying a number of rules to the data.  To simplify analysis, we convert capital letters to lower-case (to make sure that any case-sensitive string comparisons work).  Then we remove punctuation, numbers, stopwords, and extraneous whitespace.  This gives us a much simpler data set to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.clean <- corpus %>%\n",
    "  tm::tm_map(tm::content_transformer(tolower)) %>% \n",
    "  tm::tm_map(tm::removePunctuation) %>%\n",
    "  tm::tm_map(tm::removeNumbers) %>%\n",
    "  tm::tm_map(tm::removeWords, tm::stopwords(kind=\"en\")) %>%\n",
    "  tm::tm_map(tm::stripWhitespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next bit is the real trick in NLP:  converting words to features.  We're going to use the bag of words technique, converting our set of unique words into a set of features, and building a matrix of reviews versus words, with the elements being count of occurrences.\n",
    "\n",
    "Below is a sample of 6 terms as they appear in 11 documents (that is, our reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm <- tm::DocumentTermMatrix(corpus.clean)\n",
    "inspect(dtm[40:50,10:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we split out training and test data.  Please see above about how this can lead to some overfitting of the test data.\n",
    "\n",
    "Our split is 75% training and 25% test data.  You can experiment with other percentages as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.train <- df[1:1500,]\n",
    "df.test <- df[1501:2000,]\n",
    "\n",
    "dtm.train <- dtm[1:1500,]\n",
    "dtm.test <- dtm[1501:2000,]\n",
    "\n",
    "corpus.clean.train <- corpus.clean[1:1500]\n",
    "corpus.clean.test <- corpus.clean[1501:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking our input data set, there are 38,957 unique terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(dtm.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of these terms appear in only one document.  When using a technique like TF-IDF, that can be a good thing, but if you have a huge number of terms, it can quickly blow out your available memory.  We'll limit our input set to a set of frequently-used terms.  In this case, we define \"frequently used\" as existing in five separate documents.\n",
    "\n",
    "Once we do that, we're down to 12,144 unique terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiveFreq <- tm::findFreqTerms(dtm.train, 5)\n",
    "length(fiveFreq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our dictionary has changed (from the full set of terms to our frequently-used terms), we'll want to build a new pair of document term matrices, feeding in the frequently-used terms as our dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm.train.nb <- tm::DocumentTermMatrix(corpus.clean.train, control=list(dictionary = fiveFreq))\n",
    "dtm.test.nb <- tm::DocumentTermMatrix(corpus.clean.test, control=list(dictionary = fiveFreq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of occurrences of a term can be interesting, but in this analysis, we'll simplify the calculations a bit by looking for the *appearance* of a term rather than *number of appearances*.  If the term \"available\" shows up 30 times in one document, that counts as 1 occurrence rather than 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_count <- function(x) {\n",
    "  y <- ifelse(x > 0, 1,0)\n",
    "  y <- factor(y, levels=c(0,1), labels=c(\"No\", \"Yes\"))\n",
    "  y\n",
    "}\n",
    "\n",
    "trainNB <- apply(dtm.train.nb, 2, convert_count)\n",
    "testNB <- apply(dtm.test.nb, 2, convert_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our inputs are cleaned up and ready to go, let's build a classifier function and then predict using that classifier against the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier <- naivebayes::naive_bayes(trainNB, df.train$class, laplace = 1)\n",
    "pred <- predict(classifier, newdata=testNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've done that, we have a new confusion matrix.  We can see that our classifier has an accuracy of 81%.\n",
    "\n",
    "This example lets us look at four important features:  sensitivity, specificity, positive predictive value, and negative predictive value.  This is a simple two-class, binary classifier so these definitions are pretty simple.  The tricky part is that the confusion matrix in `caret` orders alphabetically, whereas ideally you want the \"positive\" result first and the \"negative\" result last. \n",
    "\n",
    "**Sensitivity** is where we capture when an event is positive, whether our predictor considers it positive, and is defined as (Rpos|Ppos) / (Rpos).  That is, 181/(181+54) or 0.7702.\n",
    "\n",
    "**Specificity** is where we capture when an event is negative, whether our predictor considers it negative, and is defined as (Rneg|Pneg) / (Rneg).  That is, 224/(224+41) or 0.8453.\n",
    "\n",
    "**Positive predictive value** looks at all cases where the Prediction was positive (read the \"Pos\" row), and is defined as (Ppos|Rpos) / (Ppos).  That is, 181/(181+41) or 0.8153.\n",
    "\n",
    "**Negative predictive value** looks at cases where the Prediction was negative (read the \"Neg\" row), and is defined as (Pneg|Rneg) / (Pneg).  That is, 224/(224+54) or 0.8058."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.mat <- caret::confusionMatrix(pred, df.test$class, positive=\"Pos\")\n",
    "conf.mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also dig into specific elements of the confusion matrix like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.mat$byClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.mat$overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.mat$overall[\"Accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
